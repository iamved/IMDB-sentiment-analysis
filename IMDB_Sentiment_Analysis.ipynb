{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB movie reviews sentiment analysis Using BidirectionalLSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a short implementation for IMDB movie reviews sentiment analysis. Its a binary classification task, where we  have a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. I used the original dataset relased by Stanford(Large Movie Review Dataset v1.0 ). I used Bidirectional LSTM on this dataset, and got a really good accuracy(Training 94%, Testing 91%). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementation I have used Keras, Pandas, NLTK. First of all here we are going to import all the libraries that we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ved/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "\n",
    "import nltk\n",
    "# nltk.download('words')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet \n",
    "allEnglishWords = words.words() + [w for w in wordnet.words()]\n",
    "allEnglishWords = np.unique([x.lower() for x in allEnglishWords])\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we are going to import our datasets. So main dataset have 2 folders for train and test set. Train and Test set both consist of two subfolders for positive and negative reviews. All the reviews are inside .txt files. So to use this dataset we have combine all these files from various folders and make a single dataset.\n",
    "Here I have imported files from all the 4 folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/ved/Desktop/Int_prep/aclImdb/\"\n",
    "positiveSamples = [x for x in os.listdir(path+\"train/pos/\") if x.endswith(\".txt\")]\n",
    "negativeSamples = [x for x in os.listdir(path+\"train/neg/\") if x.endswith(\".txt\")]\n",
    "pos_test_samples = [x for x in os.listdir(path+\"test/pos/\") if x.endswith(\".txt\")]\n",
    "neg_test_samples = [x for x in os.listdir(path+\"test/neg/\") if x.endswith(\".txt\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have saved texts from all the files into 4 different lists for each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n"
     ]
    }
   ],
   "source": [
    "positiveReviews, negativeReviews, pos_test_Reviews, neg_test_Reviews = [], [], [], []\n",
    "for pSameple in positiveSamples:\n",
    "    with open(path+\"train/pos/\"+pfile, encoding=\"latin1\") as f:\n",
    "        positiveReviews.append(f.read())\n",
    "for nSample in negativeSamples:\n",
    "    with open(path+\"train/neg/\"+nfile, encoding=\"latin1\") as f:\n",
    "        negativeReviews.append(f.read())\n",
    "for tfile in pos_test_samples:\n",
    "    with open(path+\"test/pos/\"+tfile, encoding=\"latin1\") as f:\n",
    "        pos_test_Reviews.append(f.read())\n",
    "for tfile in neg_test_samples:\n",
    "    with open(path+\"test/neg/\"+tfile, encoding=\"latin1\") as f:\n",
    "        neg_test_Reviews.append(f.read())\n",
    "print(len(pos_test_Reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that I combined these lists into single pandas dataframe. This dataframe have 2 columns, one is for review text and other one is for label. Now in total this dataframe consists 50000 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "    pd.DataFrame({\"review\":positiveReviews, \"label\":1}),\n",
    "    pd.DataFrame({\"review\":negativeReviews, \"label\":0}),\n",
    "    pd.DataFrame({\"review\":pos_test_Reviews, \"label\":1}),\n",
    "    pd.DataFrame({\"review\":neg_test_Reviews, \"label\":0})\n",
    "], ignore_index=True).sample(frac=1, random_state=1)\n",
    "\n",
    "\n",
    "len(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use our text we need to first do some preprocessing on it. We need to remove the stopwords, convert texts to lowercase, extracts lemmas from words. So here we will create a new column of processed reviews in our dataframe df1, which will consist our reviews after preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "df1['Processed_Reviews'] = df1.review.apply(lambda x: clean_text(x))\n",
    "#df2['Processed_Reviews'] = df2.review.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the main part of our task. Here we are going to first tokenize and then do the modeling for this task. I extracted tokens from the text, passed it through embedding layer to use embedding for each token, then I passed it through a Bidirectional LSTM layer. Bidirectional LSTM are really just putting two independent LSTMs together. The input sequence is fed in normal time order for one network, and in reverse time order for another. Using Bidirectional LSTM improves accuracy by good amount. We also have a dropout layer to add some regularization into the network. Also I have tried inbuilt LSTM regularizers, as a result our model performs really well and its not that overfitted. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from that there are 2 dense layer too. For optimization I have used 'Adam' optimizer which is really good as compared to other optimizers as it consider momentum also.  Activation function is Relu, which is kind of standard for LSTM, its saves us from the issue of vanishing gradient and help our network converge faster.\n",
    "Batch size is 500 and number of epochs is 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 287s 7ms/step - loss: 10.3814 - acc: 0.5449 - val_loss: 2.5126 - val_acc: 0.7652\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 277s 7ms/step - loss: 2.0946 - acc: 0.7542 - val_loss: 1.9560 - val_acc: 0.7611\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 2098s 52ms/step - loss: 1.8888 - acc: 0.7550 - val_loss: 1.8494 - val_acc: 0.7685\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 313s 8ms/step - loss: 1.7416 - acc: 0.7535 - val_loss: 1.7463 - val_acc: 0.7521\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 322s 8ms/step - loss: 1.5642 - acc: 0.7551 - val_loss: 1.5037 - val_acc: 0.7539\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 305s 8ms/step - loss: 1.3835 - acc: 0.7809 - val_loss: 1.3660 - val_acc: 0.8012\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 358s 9ms/step - loss: 1.2736 - acc: 0.8231 - val_loss: 1.2336 - val_acc: 0.8388\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 289s 7ms/step - loss: 1.2130 - acc: 0.8519 - val_loss: 1.2657 - val_acc: 0.8592\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 296s 7ms/step - loss: 1.1910 - acc: 0.8714 - val_loss: 1.1580 - val_acc: 0.8708\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 421s 11ms/step - loss: 1.0814 - acc: 0.8823 - val_loss: 1.0673 - val_acc: 0.8748\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 351s 9ms/step - loss: 1.0714 - acc: 0.8911 - val_loss: 1.1092 - val_acc: 0.8796\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 281s 7ms/step - loss: 1.0029 - acc: 0.9004 - val_loss: 1.0244 - val_acc: 0.8885\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 419s 10ms/step - loss: 0.9560 - acc: 0.9103 - val_loss: 1.0183 - val_acc: 0.8908\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 475s 12ms/step - loss: 0.9221 - acc: 0.9150 - val_loss: 0.9548 - val_acc: 0.8939\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 362s 9ms/step - loss: 0.8819 - acc: 0.9211 - val_loss: 0.8965 - val_acc: 0.8965\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 291s 7ms/step - loss: 0.8420 - acc: 0.9274 - val_loss: 0.9228 - val_acc: 0.8956\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 346s 9ms/step - loss: 0.8047 - acc: 0.9330 - val_loss: 0.8702 - val_acc: 0.8991\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 332s 8ms/step - loss: 0.7889 - acc: 0.9336 - val_loss: 0.7864 - val_acc: 0.8991\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 318s 8ms/step - loss: 0.7547 - acc: 0.9377 - val_loss: 0.8444 - val_acc: 0.9003\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 310s 8ms/step - loss: 0.7374 - acc: 0.9433 - val_loss: 0.7460 - val_acc: 0.9030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4c384da0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 8000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df1['Processed_Reviews'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(df1['Processed_Reviews'])\n",
    "\n",
    "maxlen = 130\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "y = df1['label']\n",
    "\n",
    "embed_size = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True, kernel_regularizer=regularizers.l2(0.001),\n",
    "                activity_regularizer=regularizers.l1(0.001))))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 500\n",
    "epochs = 20\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So here we have got training accuracy of 94.33% and validation accuracy of 90.30 % in just 20 epochs, which is pretty good. Model is slightly overfitted which we can imporve by proper hyerparameter tunning. By tunning regularization parameters and ruuning for few more epoch we can get really good results for both traning and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
